// code by jph
package ch.alpine.subare.book.ch04.gambler;

import ch.alpine.subare.alg.ActionValueIteration;
import ch.alpine.subare.alg.ValueIteration;
import ch.alpine.subare.api.EpisodeInterface;
import ch.alpine.subare.api.Policy;
import ch.alpine.subare.api.StepRecord;
import ch.alpine.subare.util.DiscreteQsa;
import ch.alpine.subare.util.DiscreteUtils;
import ch.alpine.subare.util.DiscreteVs;
import ch.alpine.subare.util.EpisodeKickoff;
import ch.alpine.subare.util.PolicyType;
import ch.alpine.tensor.Tensor;
import ch.alpine.tensor.sca.Chop;
import ch.alpine.tensor.sca.Round;

/* package */ enum GamblerHelper {
  ;
  static DiscreteQsa getOptimalQsa(GamblerModel gamblerModel) {
    return ActionValueIteration.solve(gamblerModel, Chop._04);
  }

  public static DiscreteVs getOptimalVs(GamblerModel gamblerModel) {
    return ValueIteration.solve(gamblerModel, Chop._10);
  }

  public static Policy getOptimalPolicy(GamblerModel gamblerModel) {
    // TODO SUBARE test for equality of policies from qsa and vs
    ValueIteration vi = new ValueIteration(gamblerModel, gamblerModel);
    vi.untilBelow(Chop._10);
    return PolicyType.GREEDY.bestEquiprobable(gamblerModel, vi.vs(), null);
  }

  public static void play(GamblerModel gamblerModel, DiscreteQsa qsa) {
    DiscreteUtils.print(qsa, Round._2);
    System.out.println("---");
    Policy policy = PolicyType.GREEDY.bestEquiprobable(gamblerModel, qsa, null);
    EpisodeInterface mce = EpisodeKickoff.single(gamblerModel, policy, //
        gamblerModel.startStates().get(gamblerModel.startStates().length() / 2));
    while (mce.hasNext()) {
      StepRecord stepRecord = mce.step();
      Tensor state = stepRecord.prevState();
      System.out.println(state + " then " + stepRecord.action());
    }
  }
}
